{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle data format uses a relatively compact binary representation.\n",
    "The pickle module implements binary protocols for serializing and de-serializing a Python object structure.Pickling is the process whereby a Python object hierarchy is converted into a byte stream, and unpickling is the inverse operation, whereby a byte stream is converted back into an object hierarchy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "1.features -> the images pixel values, (width, height, channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.labels -> the label of the traffic sign\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.sizes -> the original width and height of the image, (width, height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "training_file = \"data/train.p\"\n",
    "testing_file = \"data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Summary & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "#Number of testing examples.\n",
    "n_test = len(X_test)\n",
    "\n",
    "#shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "#How many unique labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# show image of 10 random data points\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
    "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "for i in range(10):\n",
    "    index = random.randint(0, len(X_train))\n",
    "    image = X_train[index]\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_title(y_train[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of label frequency\n",
    "import matplotlib.pyplot as plt\n",
    "hist, bins = np.histogram(y_train, bins=n_classes)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Design and Test a Model Architecture\n",
    "  Conversion of data to grayscale is not necessary for image processing,but it is done for a few reasons:-\n",
    "  1.Simplicity:-If you have an RGBA image you might need to apply the operation on each of the four image planes and then combine the results. Gray scale images only contain one image plane \n",
    "  2.Data Reduction:-If you have a RGBA image (red-green-blue-alpha). If you converted this image to gray scale you would only need to process 1/4 of the data compared to the color image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of data .\n",
    "#### Data preprocessing is a technique that involves transforming raw data into an understandable format. It prepares raw data for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to grayscale\n",
    "X_train_rgb = X_train\n",
    "X_train_gry = np.sum(X_train/3, axis=3, keepdims=True)\n",
    "\n",
    "X_test_rgb = X_test\n",
    "X_test_gry = np.sum(X_test/3, axis=3, keepdims=True)\n",
    "\n",
    "print('RGB shape:', X_train_rgb.shape)\n",
    "print('Grayscale shape:', X_train_gry.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_gry\n",
    "X_test = X_test_gry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize rgb vs grayscale\n",
    "n_rows = 8\n",
    "n_cols = 10\n",
    "offset = 9000\n",
    "fig, axs = plt.subplots(n_rows,n_cols, figsize=(18, 14))\n",
    "fig.subplots_adjust(hspace = .1, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "for j in range(0,n_rows,2):\n",
    "    for i in range(n_cols):\n",
    "        index = i + j*n_cols\n",
    "        image = X_train_rgb[index + offset]\n",
    "        axs[index].axis('off')\n",
    "        axs[index].imshow(image)\n",
    "    for i in range(n_cols):\n",
    "        index = i + j*n_cols + n_cols \n",
    "        image = X_train_gry[index + offset - n_cols].squeeze()\n",
    "        axs[index].axis('off')\n",
    "        axs[index].imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_train))\n",
    "print(np.mean(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling is used to bring all values into the range [0,1]. This is also called unity-based normalization. This can be generalized to restrict the range of values in the dataset between any arbitrary points and using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the train and test datasets to (-1,1)\n",
    "\n",
    "X_train_normalized = (X_train - 128)/128 \n",
    "X_test_normalized = (X_test - 128)/128\n",
    "\n",
    "print(np.mean(X_train_normalized))\n",
    "print(np.mean(X_test_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Original shape:\", X_train.shape)\n",
    "print(\"Normalized shape:\", X_train_normalized.shape)\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 3))\n",
    "axs = axs.ravel()\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('normalized')\n",
    "axs[0].imshow(X_train_normalized[0].squeeze(), cmap='gray')\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('original')\n",
    "axs[1].imshow(X_train[0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def random_translate(img):\n",
    "    rows,cols,_ = img.shape\n",
    "    \n",
    "    # allow translation up to px pixels in x and y directions\n",
    "    px = 2\n",
    "    dx,dy = np.random.randint(-px,px,2)\n",
    "\n",
    "    M = np.float32([[1,0,dx],[0,1,dy]])\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    \n",
    "    dst = dst[:,:,np.newaxis]\n",
    "    \n",
    "    return dst\n",
    "\n",
    "test_img = X_train_normalized[22222]\n",
    "\n",
    "test_dst = random_translate(test_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(test_img.squeeze(), cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(test_dst.squeeze(), cmap='gray')\n",
    "axs[1].set_title('translated')\n",
    "\n",
    "print('shape in/out:', test_img.shape, test_dst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_scaling(img):   \n",
    "    rows,cols,_ = img.shape\n",
    "\n",
    "    # transform limits\n",
    "    px = np.random.randint(-2,2)\n",
    "\n",
    "    # ending locations\n",
    "    pts1 = np.float32([[px,px],[rows-px,px],[px,cols-px],[rows-px,cols-px]])\n",
    "\n",
    "    # starting locations (4 corners)\n",
    "    pts2 = np.float32([[0,0],[rows,0],[0,cols],[rows,cols]])\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "    dst = cv2.warpPerspective(img,M,(rows,cols))\n",
    "    \n",
    "    dst = dst[:,:,np.newaxis]\n",
    "    \n",
    "    return dst\n",
    "\n",
    "test_dst = random_scaling(test_img)\n",
    "    \n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(test_img.squeeze(), cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(test_dst.squeeze(), cmap='gray')\n",
    "axs[1].set_title('scaled')\n",
    "\n",
    "print('shape in/out:', test_img.shape, test_dst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_warp(img):\n",
    "    \n",
    "    rows,cols,_ = img.shape\n",
    "\n",
    "    # random scaling coefficients\n",
    "    rndx = np.random.rand(3) - 0.5\n",
    "    rndx *= cols * 0.06   # this coefficient determines the degree of warping\n",
    "    rndy = np.random.rand(3) - 0.5\n",
    "    rndy *= rows * 0.06\n",
    "\n",
    "    # 3 starting points for transform, 1/4 way from edges\n",
    "    x1 = cols/4\n",
    "    x2 = 3*cols/4\n",
    "    y1 = rows/4\n",
    "    y2 = 3*rows/4\n",
    "\n",
    "    pts1 = np.float32([[y1,x1],\n",
    "                       [y2,x1],\n",
    "                       [y1,x2]])\n",
    "    pts2 = np.float32([[y1+rndy[0],x1+rndx[0]],\n",
    "                       [y2+rndy[1],x1+rndx[1]],\n",
    "                       [y1+rndy[2],x2+rndx[2]]])\n",
    "\n",
    "    M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    \n",
    "    dst = dst[:,:,np.newaxis]\n",
    "    \n",
    "    return dst\n",
    "\n",
    "test_dst = random_warp(test_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(test_img.squeeze(), cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(test_dst.squeeze(), cmap='gray')\n",
    "axs[1].set_title('warped')\n",
    "\n",
    "print('shape in/out:', test_img.shape, test_dst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_brightness(img):\n",
    "    shifted = img + 1.0   # shift to (0,2) range\n",
    "    img_max_value = max(shifted.flatten())\n",
    "    max_coef = 2.0/img_max_value\n",
    "    min_coef = max_coef - 0.1\n",
    "    coef = np.random.uniform(min_coef, max_coef)\n",
    "    dst = shifted * coef - 1.0\n",
    "    return dst\n",
    "\n",
    "test_dst = random_brightness(test_img)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 3))\n",
    "\n",
    "axs[0].axis('off')\n",
    "axs[0].imshow(test_img.squeeze(), cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "\n",
    "axs[1].axis('off')\n",
    "axs[1].imshow(test_dst.squeeze(), cmap='gray')\n",
    "axs[1].set_title('brightness adjusted')\n",
    "\n",
    "print('shape in/out:', test_img.shape, test_dst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of label frequency \n",
    "hist, bins = np.histogram(y_train, bins=n_classes)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "print(\"minimum samples for any label:\", min(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X, y shapes:', X_train_normalized.shape, y_train.shape)\n",
    "\n",
    "input_indices = []\n",
    "output_indices = []\n",
    "\n",
    "for class_n in range(n_classes):\n",
    "    print(class_n, ': ', end='')\n",
    "    class_indices = np.where(y_train == class_n)\n",
    "    n_samples = len(class_indices[0])\n",
    "    if n_samples < 800:\n",
    "        for i in range(800 - n_samples):\n",
    "            input_indices.append(class_indices[0][i%n_samples])\n",
    "            output_indices.append(X_train_normalized.shape[0])\n",
    "            new_img = X_train_normalized[class_indices[0][i % n_samples]]\n",
    "            new_img = random_translate(random_scaling(random_warp(random_brightness(new_img))))\n",
    "            X_train_normalized = np.concatenate((X_train_normalized, [new_img]), axis=0)\n",
    "            y_train = np.concatenate((y_train, [class_n]), axis=0)\n",
    "            if i % 50 == 0:\n",
    "                print('|', end='')\n",
    "            elif i % 10 == 0:\n",
    "                print('-',end='')\n",
    "    print('')\n",
    "            \n",
    "print('X, y shapes:', X_train_normalized.shape, y_train.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparisons of 5 random augmented data points\n",
    "choices = list(range(len(input_indices)))\n",
    "picks = []\n",
    "for i in range(5):\n",
    "    rnd_index = np.random.randint(low=0,high=len(choices))\n",
    "    picks.append(choices.pop(rnd_index))\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
    "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "for i in range(5):\n",
    "    image = X_train_normalized[input_indices[picks[i]]].squeeze()\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(image, cmap = 'gray')\n",
    "    axs[i].set_title(y_train[input_indices[picks[i]]])\n",
    "for i in range(5):\n",
    "    image = X_train_normalized[output_indices[picks[i]]].squeeze()\n",
    "    axs[i+5].axis('off')\n",
    "    axs[i+5].imshow(image, cmap = 'gray')\n",
    "    axs[i+5].set_title(y_train[output_indices[picks[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of label frequency\n",
    "hist, bins = np.histogram(y_train, bins=n_classes)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Shuffle the training dataset\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train_normalized, y_train = shuffle(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in training ,validation and testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_normalized, y_train, \n",
    "                                                                test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Old X_train size:\",len(X_train_normalized))\n",
    "print(\"New X_train size:\",len(X_train))\n",
    "print(\"X_validation size:\",len(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet Model Architecture\n",
    "  The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs â€” it can even run on the CPU (if your system does not have a suitable GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    #Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma), name=\"W1\")\n",
    "    x = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b1 = tf.Variable(tf.zeros(6), name=\"b1\")\n",
    "    x = tf.nn.bias_add(x, b1)\n",
    "    print(\"layer 1 shape:\",x.get_shape())\n",
    "\n",
    "    # Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer1 = x\n",
    "    \n",
    "    #Layer 2: Convolutional. Output = 10x10x16.\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma), name=\"W2\")\n",
    "    x = tf.nn.conv2d(x, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b2 = tf.Variable(tf.zeros(16), name=\"b2\")\n",
    "    x = tf.nn.bias_add(x, b2)\n",
    "                     \n",
    "    #Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer2 = x\n",
    "    \n",
    "    #Layer 3: Convolutional. Output = 1x1x400.\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 400), mean = mu, stddev = sigma), name=\"W3\")\n",
    "    x = tf.nn.conv2d(x, W3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b3 = tf.Variable(tf.zeros(400), name=\"b3\")\n",
    "    x = tf.nn.bias_add(x, b3)\n",
    "                     \n",
    "    # Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    layer3 = x\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    layer2flat = flatten(layer2)\n",
    "    print(\"layer2flat shape:\",layer2flat.get_shape())\n",
    "    \n",
    "    # Flatten x. Input = 1x1x400. Output = 400.\n",
    "    xflat = flatten(x)\n",
    "    print(\"xflat shape:\",xflat.get_shape())\n",
    "    \n",
    "    # Concat layer2flat and x. Input = 400 + 400. Output = 800\n",
    "    x = tf.concat([xflat, layer2flat], 1)\n",
    "    print(\"x shape:\",x.get_shape())\n",
    "    \n",
    "    # Dropout\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 800. Output = 43.\n",
    "    W4 = tf.Variable(tf.truncated_normal(shape=(800, 43), mean = mu, stddev = sigma), name=\"W4\")\n",
    "    b4 = tf.Variable(tf.zeros(43), name=\"b4\")    \n",
    "    logits = tf.add(tf.matmul(x, W4), b4)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training of model\n",
    " Tensor flow is a system for building and training neural networks to detect and decipher patterns and correlations, analogous to human learning and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.0009\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, 'lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver2 = tf.train.import_meta_graph('./lenet.meta')\n",
    "    saver2.restore(sess, \"./lenet\")\n",
    "    test_accuracy = evaluate(X_test_normalized, y_test)\n",
    "    print(\"Test Set Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Model on New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "\n",
    "#reading in an image\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig, axs = plt.subplots(2,4, figsize=(4, 2))\n",
    "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "my_images = []\n",
    "\n",
    "for i, img in enumerate(glob.glob('./my-found-traffic-signs/*x.png')):\n",
    "    image = cv2.imread(img)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    my_images.append(image)\n",
    "\n",
    "my_images = np.asarray(my_images)\n",
    "\n",
    "my_images_gry = np.sum(my_images/3, axis=3, keepdims=True)\n",
    "\n",
    "my_images_normalized = (my_images_gry - 128)/128 \n",
    "\n",
    "print(my_images_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_labels = [3, 11, 1, 12, 38, 34, 18, 25]\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver3 = tf.train.import_meta_graph('./lenet.meta')\n",
    "    saver3.restore(sess, \"./lenet\")\n",
    "    my_accuracy = evaluate(my_images_normalized, my_labels)\n",
    "    print(\"Test Set Accuracy = {:.3f}\".format(my_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #the softmax function, or normalized exponential function, generalization of the logistic function that \"squashes\" a K-dimensional vector {\\displaystyle \\mathbf {z} } \\mathbf {z}  of arbitrary real values to a K-dimensional vector {\\displaystyle \\sigma (\\mathbf {z} )} \\sigma (\\mathbf {z} ) of real values in the range [0, 1] that add up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Visualizing the softmax probabilities\n",
    "softmax_logits = tf.nn.softmax(logits)\n",
    "top_k = tf.nn.top_k(softmax_logits, k=3)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph('./lenet.meta')\n",
    "    saver.restore(sess, \"./lenet\")\n",
    "    my_softmax_logits = sess.run(softmax_logits, feed_dict={x: my_images_normalized, keep_prob: 1.0})\n",
    "    my_top_k = sess.run(top_k, feed_dict={x: my_images_normalized, keep_prob: 1.0})\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(len(my_images),4, figsize=(12, 14))\n",
    "    fig.subplots_adjust(hspace = .4, wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, image in enumerate(my_images):\n",
    "        axs[4*i].axis('off')\n",
    "        axs[4*i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        axs[4*i].set_title('input')\n",
    "        guess1 = my_top_k[1][i][0]\n",
    "        index1 = np.argwhere(y_validation == guess1)[0]\n",
    "        axs[4*i+1].axis('off')\n",
    "        axs[4*i+1].imshow(X_validation[index1].squeeze(), cmap='gray')\n",
    "        axs[4*i+1].set_title('top guess: {} ({:.0f}%)'.format(guess1, 100*my_top_k[0][i][0]))\n",
    "        guess2 = my_top_k[1][i][1]\n",
    "        index2 = np.argwhere(y_validation == guess2)[0]\n",
    "        axs[4*i+2].axis('off')\n",
    "        axs[4*i+2].imshow(X_validation[index2].squeeze(), cmap='gray')\n",
    "        axs[4*i+2].set_title('2nd guess: {} ({:.0f}%)'.format(guess2, 100*my_top_k[0][i][1]))\n",
    "        guess3 = my_top_k[1][i][2]\n",
    "        index3 = np.argwhere(y_validation == guess3)[0]\n",
    "        axs[4*i+3].axis('off')\n",
    "        axs[4*i+3].imshow(X_validation[index3].squeeze(), cmap='gray')\n",
    "        axs[4*i+3].set_title('3rd guess: {} ({:.0f}%)'.format(guess3, 100*my_top_k[0][i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8,2, figsize=(9, 19))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(len(my_softmax_logits)*2):\n",
    "    if i%2 == 0:\n",
    "        axs[i].axis('off')\n",
    "        axs[i].imshow(cv2.cvtColor(my_images[i//2], cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        axs[i].bar(np.arange(n_classes), my_softmax_logits[(i-1)//2]) \n",
    "        axs[i].set_ylabel('Softmax probability')\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
