{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title:Traffic Sign Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from multiprocessing import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pickle data format uses a relatively compact binary representation.\n",
    "The pickle module implements binary protocols for serializing and de-serializing a Python object structure.Pickling is the process whereby a Python object hierarchy is converted into a byte stream, and unpickling is the inverse operation, whereby a byte stream is converted back into an object hierarchy. \n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.features -> the images pixel values, (width, height, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.labels -> the label of the traffic sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.sizes -> the original width and height of the image, (width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load pickled data\n",
    "import pickle\n",
    "\n",
    "training_file = \"traffic-signs-data/train.p\"\n",
    "testing_file = \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data summary.\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot four sample images\n",
    "print('Sample images')\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(X_train[i*1500+1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the count of the number of examples of each sign\n",
    "plt.hist(y_train, bins=n_classes)\n",
    "plt.title('Number of eg of each sign in given training set')\n",
    "plt.xlabel('Various Sign')\n",
    "plt.ylabel('Count of Signs')\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[17031])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Data PreProcessing\n",
    "Data preprocessing is a technique that involves transforming raw data into an understandable format. It prepares raw data for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle training examples\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_orig = X_train\n",
    "X_test_orig = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalise input (images still in colour)\n",
    "X_train = (X_train - X_train.mean()) / (np.max(X_train) - np.min(X_train))\n",
    "X_test = (X_test - X_test.mean()) / (np.max(X_test) - np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_norm_image(image_index):\n",
    "    \"\"\"Plots original image on the left and normalised image on the right.\"\"\"\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(X_train_orig[image_index])\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(X_train[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_norm_image(9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Split data in training ,validation and testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "n_input = 32 * 32 * 3\n",
    "nb_filters = 32\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (32, 32, 3)\n",
    "n_fc1 = 512\n",
    "n_fc2 = 128\n",
    "in_channels = 3\n",
    "pool_size = 2 # i.e. (2,2)\n",
    "\n",
    "dropout_conv = 0.9\n",
    "dropout_fc = 0.9\n",
    "\n",
    "weights_stddev = 0.1\n",
    "weights_mean = 0.0\n",
    "biases_mean = 0.0\n",
    "\n",
    "padding = 'VALID'\n",
    "if padding == 'SAME':\n",
    "    conv_output_length = 6\n",
    "elif padding == 'VALID':\n",
    "    conv_output_length = 5\n",
    "else:\n",
    "    raiseException(\"Unknown padding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x_unflattened = tf.placeholder(\"float\", [None, 32, 32, 3])\n",
    "x = x_unflattened\n",
    "\n",
    "y_rawlabels = tf.placeholder(\"int32\", [None])\n",
    "y = tf.one_hot(y_rawlabels, depth=43, on_value=1., off_value=0., axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A convolutional neural network (CNN, or ConvNet)is a class of deep, feed-forward artificialneural networks that has successfully been applied to analyzing visual imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "def conv2d(x, W, b, strides=3):\n",
    "    \"\"\"Conv2D wrapper, with bias and relu activation\"\"\"\n",
    "    # strides = [batch, in_height, in_width, channels]\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2, padding_setting='SAME'):\n",
    "    \"\"\"MaxPool2D wrapper.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding=padding_setting)\n",
    "\n",
    "def conv_net(model_x, model_weights, model_biases, model_pool_size, \n",
    "             model_dropout_conv, model_dropout_fc, padding='SAME'):\n",
    "    \"\"\"Convolutional neural network model.\"\"\"\n",
    "    # Convolution Layer 1\n",
    "    conv1 = conv2d(model_x, model_weights['conv1'], model_biases['conv1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=model_pool_size, padding_setting=padding)\n",
    "    conv1 = tf.nn.dropout(conv1, model_dropout_conv)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    # Reshape conv1 output to fit fully connected layer input\n",
    "    conv1_shape = conv1.get_shape().as_list()\n",
    "    fc1 = tf.reshape(conv1, [-1, conv1_shape[1]*conv1_shape[2]*conv1_shape[3]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, model_weights['fc1']), model_biases['fc1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, model_dropout_fc)\n",
    "    # Fully connected layer 2\n",
    "    fc2 = tf.add(tf.matmul(fc1, model_weights['fc2']), model_biases['fc2'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, model_dropout_fc)\n",
    "    # Output layer\n",
    "    output = tf.add(tf.matmul(fc2, model_weights['out']), model_biases['out'])\n",
    "    # Note: Softmax is outside the model\n",
    "    return output\n",
    "\n",
    "\n",
    "## Store layers weight & bias\n",
    "\n",
    "# NEW: initialise neurons with slightly positive initial bias\n",
    "# to avoid dead neurons.\n",
    "def weight_variable(shape, weight_mean, weight_stddev):\n",
    "    initial = tf.truncated_normal(shape, stddev=weight_stddev, mean=weight_mean)\n",
    "    # alt: tf.random_normal(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape, bias_mean):\n",
    "    initial = tf.constant(bias_mean, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'conv1': weight_variable([kernel_size[0], kernel_size[1], in_channels, nb_filters], weights_mean, weights_stddev),\n",
    "    'fc1': weight_variable([nb_filters * conv_output_length**2, n_fc1], weights_mean, weights_stddev),\n",
    "    'fc2': weight_variable([n_fc1, n_fc2], weights_mean, weights_stddev),\n",
    "    'out': weight_variable([n_fc2, n_classes], weights_mean, weights_stddev)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1': bias_variable([nb_filters], biases_mean),\n",
    "    'fc1': bias_variable([n_fc1], biases_mean),\n",
    "    'fc2': bias_variable([n_fc2], biases_mean),\n",
    "    'out': bias_variable([n_classes], biases_mean)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training Model\n",
    " Tensor flow is a system for building and training neural networks to detect and decipher patterns and correlations, analogous to human learning and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "initial_learning_rate = learning_rate\n",
    "training_epochs = 150\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "n_train = len(X_train)\n",
    "\n",
    "anneal_mod_frequency = 15\n",
    "# Annealing rate of 1: learning rate remains constant.\n",
    "annealing_rate = 1\n",
    "\n",
    "print_accuracy_mod_frequency = 1\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, pool_size, dropout_conv, dropout_fc, padding=padding)\n",
    "pred_probs = tf.nn.softmax(pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Function to initialise the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "### RUN MODEL ###\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialise variables\n",
    "sess.run(init)\n",
    "\n",
    "# Initialise time logs\n",
    "init_time = time.time()\n",
    "epoch_time = init_time\n",
    "\n",
    "five_epoch_moving_average = 0.\n",
    "epoch_accuracies = []\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    if five_epoch_moving_average > 0.96:\n",
    "        break\n",
    "        \n",
    "    avg_cost = 0.\n",
    "\n",
    "    total_batch = int(n_train / batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = np.array(X_train[i * batch_size:(i + 1) * batch_size]), \\\n",
    "                           np.array(y_train[i * batch_size:(i + 1) * batch_size])\n",
    "        \n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x_unflattened: batch_x, y_rawlabels: batch_y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "        # print(avg_cost)\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\",\n",
    "              \"{:.9f}\".format(avg_cost))\n",
    "        last_epoch_time = epoch_time\n",
    "        epoch_time = time.time()\n",
    "        # print(\"Time since last epoch: \", epoch_time - last_epoch_time)\n",
    "    # Anneal learning rate\n",
    "    if (epoch + 1) % anneal_mod_frequency == 0:\n",
    "        learning_rate *= annealing_rate\n",
    "        print(\"New learning rate: \", learning_rate)\n",
    "\n",
    "    if (epoch + 1) % print_accuracy_mod_frequency == 0:\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # Line below needed only when not using `with tf.Session() as sess`\n",
    "        with sess.as_default():\n",
    "            epoch_accuracy = accuracy.eval({x_unflattened: X_val, y_rawlabels: y_val})\n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            if epoch >= 4:\n",
    "                five_epoch_moving_average = np.sum(epoch_accuracies[epoch-5:epoch]) / 5\n",
    "                print(\"Five epoch moving average: \", five_epoch_moving_average)\n",
    "            print(\"Accuracy (validation):\", epoch_accuracy)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "train_predict_time = time.time()\n",
    "# print(\"Time to calculate accuracy on training set: \", train_predict_time - epoch_time)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "with sess.as_default():\n",
    "    print(\"Accuracy (test):\", accuracy.eval({x_unflattened: X_test, y_rawlabels: y_test}))\n",
    "test_predict_time = time.time()\n",
    "print(\"Time to calculate accuracy on test set: \", test_predict_time - train_predict_time)\n",
    "\n",
    "# Print parameters for reference\n",
    "print(\"\\nParameters:\")\n",
    "print(\"Learning rate (initial): \", initial_learning_rate)\n",
    "print(\"Anneal learning rate every \", anneal_mod_frequency, \" epochs by \", 1 - annealing_rate)\n",
    "print(\"Learning rate (final): \", learning_rate)\n",
    "print(\"Training epochs: \", training_epochs)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Dropout (conv): \", dropout_conv)\n",
    "print(\"Dropout (fc): \", dropout_fc)\n",
    "print(\"Padding: \", padding)\n",
    "print(\"weights_mean: \", weights_mean)\n",
    "print(\"weights_stddev: \", weights_stddev)\n",
    "print(\"biases_mean: \", biases_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Testing of Model on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "def read_image_and_print_dims(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    #printing stats and plotting\n",
    "    print('This image is:', type(image), 'with dimensions:', image.shape)\n",
    "    plt.imshow(image)  \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    \"\"\"Print model's prediction of which traffic sign this image is.\"\"\"\n",
    "    classification = sess.run(tf.argmax(pred, 1), feed_dict={x_unflattened: [img]})\n",
    "    print(classification)\n",
    "    print('Neural Network predicted', classification[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_and_pred_image(image):\n",
    "    \"\"\"Show image and print model's prediction (of which traffic sign this image is).\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_and_pred_X_train(index):\n",
    "    \"\"\"Show image from training set and print model's prediction (of which traffic sign this image is).\n",
    "    \"\"\"\n",
    "    plt.imshow(X_train[index])\n",
    "    predict(X_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_pred_X_train(30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model's Prediction on New Images\n",
    "It will be completed in next update."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
